{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a68d3c6-0885-44ea-865a-e10b840bb05a",
   "metadata": {
    "id": "4a68d3c6-0885-44ea-865a-e10b840bb05a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import re\n",
    "import pathlib\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OyxVkC4Czz2Y",
   "metadata": {
    "id": "OyxVkC4Czz2Y"
   },
   "source": [
    "# Hemingway's Haiku\n",
    "\n",
    "### Generating Haiku poems with Hemingway's writing\n",
    "\n",
    "> A haiku is a short, unrhymed Japanese poem that is written in three lines of five, seven, and five syllables, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T9nrGp-Pzuui",
   "metadata": {
    "id": "T9nrGp-Pzuui"
   },
   "source": [
    "### Workflow\n",
    "1. Create a simple text generating model (based on DLWP and provided notebooks)\n",
    "2. Evaluate the model\n",
    "3. Implementing different methods\n",
    "4. Evaluate generated text\n",
    "5. Back to step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q3eo7otpBdug",
   "metadata": {
    "id": "Q3eo7otpBdug"
   },
   "source": [
    "### 1. Simple text generating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b574da1b-3021-49fa-9140-54495c1cc013",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b574da1b-3021-49fa-9140-54495c1cc013",
    "outputId": "f16074b4-9c5e-4ae6-da29-a6fcbb5fdd47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 2  4  7  3  2 10  7  6 11  4  2  7  5 23 23 18  2 13  8 20  3  2  6 20\n",
      "  2 20 11  5  9 19  8 10  2 17  5 19  6 17 21  3 11  2  8  4  2 15  5 10\n",
      "  2  9  6 15  2 13 14  9 19  7  2  4  8 17  3  2  5  9 12  2  4  7  3 18\n",
      "  2 15  3 11  3  2  5 13 13  2 10  8  4  4  8  9 16  2 14  9 12  3 11  2\n",
      "  4  7  3  2 12], shape=(101,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# open text file of Hemingway's text\n",
    "with open(\"../content/hemingwayshorts.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read().lower()\n",
    "text = re.sub(r\"[^\\w\\s]\", \"\", text)  # remove punctuation\n",
    "\n",
    "# tokenizer layer setup\n",
    "text_vectorization = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"character\",\n",
    "    output_mode=\"int\",\n",
    ")\n",
    "\n",
    "text_vectorization.adapt([text])\n",
    "TOKEN_INDEX = dict(enumerate(text_vectorization.get_vocabulary()))\n",
    "VOCAB_SIZE = len(text_vectorization.get_vocabulary())   # retrieve the vocab size afterwards\n",
    "\n",
    "lm_dataset_raw = tf.data.Dataset.from_tensor_slices([text])\n",
    "\n",
    "lm_dataset_tok = lm_dataset_raw.map(text_vectorization)\n",
    "\n",
    "for t in lm_dataset_tok:\n",
    "    # print(t)\n",
    "    DATASET_LENGTH = t.shape[0]\n",
    "\n",
    "# tokenizing\n",
    "lm_dataset_flat = lm_dataset_tok.flat_map(\n",
    "    lambda x: tf.data.Dataset.from_tensor_slices(x)\n",
    ")\n",
    "\n",
    "SEQUENCE_LENGTH = 100\n",
    "\n",
    "lm_dataset_seqs = lm_dataset_flat.batch(\n",
    "    SEQUENCE_LENGTH + 1,\n",
    "    drop_remainder=True\n",
    ")\n",
    "\n",
    "for t in lm_dataset_seqs.take(1):\n",
    "    print(t)\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "lm_dataset_batched = (\n",
    "    lm_dataset_seqs\n",
    "        .repeat()\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE, drop_remainder=True)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# for t in lm_dataset_batched.take(1):\n",
    "#     print(t.shape)\n",
    "\n",
    "def prepare_lm_dataset(tokens_batch):\n",
    "    x = tokens_batch[:, :-1]  # [a b c d e f g] the model predicts top to bottom,\n",
    "    y = tokens_batch[:, 1:]   # [b c d e f g h] a → b, a b → c, a b c → d, ..., in one go!\n",
    "    return x, y\n",
    "\n",
    "lm_dataset = lm_dataset_batched.map(prepare_lm_dataset, num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "328108dd-c2dc-4804-930d-7442b927afef",
   "metadata": {
    "collapsed": true,
    "id": "328108dd-c2dc-4804-930d-7442b927afef",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(\"positional_embedding\")\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.token_embeddings =tf.keras.layers.Embedding(\n",
    "            input_dim=self.input_dim, output_dim=self.output_dim\n",
    "        )\n",
    "        # position embeddings: syntactic (spatial/temporal) information\n",
    "        self.position_embeddings =tf.keras.layers.Embedding(\n",
    "            input_dim=self.sequence_length, output_dim=self.output_dim\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # token embeddings: semantic information\n",
    "        self.token_embeddings =tf.keras.layers.Embedding(\n",
    "            input_dim=self.input_dim, output_dim=self.output_dim\n",
    "        )\n",
    "        # position embeddings: syntactic (spatial/temporal) information\n",
    "        self.position_embeddings =tf.keras.layers.Embedding(\n",
    "            input_dim=self.sequence_length, output_dim=self.output_dim\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        positions = tf.range(start=0, limit=length, delta=1) # delta: step size\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        # both embeddings are simply added together!\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return self.token_embeddings.compute_mask(inputs, mask=mask)\n",
    "\n",
    "    def get_config(self): # retrieve config as a dict\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def get_causal_attention_mask(inputs):\n",
    "    # print(\"Inputs:\")\n",
    "    # print(inputs)\n",
    "    # print()\n",
    "    input_shape = tf.shape(inputs)\n",
    "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "    j = tf.range(sequence_length)\n",
    "    # print(f\"i:\\n{i}\")\n",
    "    # print()\n",
    "    # print(f\"j:\\n{j}\")\n",
    "    # print()\n",
    "    mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "    # print(\"Is i >= j? Boolean cast to ints. (Note the broadcasting)\")\n",
    "    # print()\n",
    "    # print(mask)\n",
    "    # print()\n",
    "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1])) # adding a batch dimension\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1),\n",
    "         tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "    # print(\"We want mask to have the same dims as input, using `tf.tile`.\")\n",
    "    # print(\"Creating the right multiplier for it:\")\n",
    "    # print()\n",
    "    # print(mult)\n",
    "    # print()\n",
    "    tile = tf.tile(mask, mult)\n",
    "    # print(\"Final mask with batch dimensions:\")\n",
    "    # print()\n",
    "    # print(tile)\n",
    "    return tile\n",
    "\n",
    "mask = get_causal_attention_mask(tf.random.uniform(shape=(2,10), maxval=50, dtype=tf.int32))\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(\"transformer_decoder\")\n",
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "\n",
    "    # simplified class: we don't need two attention layers as we don't have data\n",
    "    # flowing from an encoder!\n",
    "\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim                              # parameters\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.supports_masking = True                            # MASK: enforcing causality\n",
    "\n",
    "    # new in Keras 3, see: https://keras.io/guides/making_new_layers_and_models_via_subclassing/#best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known\n",
    "    def build(self, input_shape):\n",
    "        self.attention_1 = tf.keras.layers.MultiHeadAttention(  # multi-head attention\n",
    "            num_heads=self.num_heads, key_dim=self.embed_dim\n",
    "        )\n",
    "        self.dense_proj = tf.keras.Sequential(                  # dense layer on top: like a nonlinearity\n",
    "            [tf.keras.layers.Dense(self.dense_dim, activation=\"relu\"),\n",
    "             tf.keras.layers.Dense(self.embed_dim),\n",
    "             tf.keras.layers.Dropout(0.1)]\n",
    "        )\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization() # layer norm\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "\n",
    "    # retrieve config as a dict (necessary for custom Keras layers)\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "\n",
    "        # prepare the causal mask\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "\n",
    "        # REGULAR MASKED ATTENTION\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask) # apply the causal mask\n",
    "\n",
    "        # residual / layer norm\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        # dense net / nonlinearity layer norm /\n",
    "        proj_output = self.layernorm_2(self.dense_proj(attention_output_1))\n",
    "\n",
    "        # residual\n",
    "        return attention_output_1 + proj_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a72947a-ee55-4dd6-9611-e0f7ef960a55",
   "metadata": {
    "id": "0a72947a-ee55-4dd6-9611-e0f7ef960a55"
   },
   "outputs": [],
   "source": [
    "EMBED_DIM = 256\n",
    "LATENT_DIM = 2048\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "def build_model(embed_dim, latent_dim, num_heads, num_layers):\n",
    "    inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    x = PositionalEmbedding(SEQUENCE_LENGTH, VOCAB_SIZE, embed_dim)(inputs)\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerDecoder(embed_dim, latent_dim, num_heads)(inputs=x) # no encoder input!\n",
    "    outputs =tf.keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)    # probability distribution over the vocab\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=tf.keras.optimizers.RMSprop(LEARNING_RATE),\n",
    "        metrics= [\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model1 = build_model(EMBED_DIM, LATENT_DIM, NUM_HEADS, NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nlj3RYKp5sia",
   "metadata": {
    "id": "nlj3RYKp5sia"
   },
   "outputs": [],
   "source": [
    "def sample_next(predictions, temperature=1.0):\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    predictions = np.log(predictions) / temperature                 # temperature reweighting\n",
    "    exp_preds = np.exp(predictions)                                 # these two lines are actually\n",
    "    predictions = exp_preds / np.sum(exp_preds)                     # a softmax\n",
    "    probas = np.random.multinomial(1, predictions, 1)               # sampling using our probabilities\n",
    "    return np.argmax(probas)\n",
    "\n",
    "class TextGenerator(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,\n",
    "                 prompt,                                            # initial context\n",
    "                 generate_length,                                   # how many words to generate\n",
    "                 seq_length,\n",
    "                 temperatures=(1.,),                                # a range of different temperatures\n",
    "                 print_every=50):\n",
    "        self.prompt = prompt\n",
    "        self.generate_length = generate_length\n",
    "        self.seq_length = seq_length\n",
    "        self.temperatures = temperatures\n",
    "        self.print_every = print_every\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch == 0 or (epoch + 1) % self.print_every == 0:\n",
    "            print()\n",
    "            print()\n",
    "            print(\"EPOCH\", epoch + 1)\n",
    "            print()\n",
    "            print(\"-\" * 40)\n",
    "            for temperature in self.temperatures:\n",
    "                msg = f\"temperature {temperature}\"\n",
    "                print(msg)\n",
    "                print(\"-\" * len(msg))\n",
    "                sentence = self.prompt                                                      # start with our prompt\n",
    "                for i in range(self.generate_length):\n",
    "                    tokenized_sentence = text_vectorization([sentence])                     # encode the sentence & feed to the model\n",
    "                    predictions = self.model(tokenized_sentence[:, - self.seq_length + 1:]) # which gives us predictions (crop to seq_len!)\n",
    "                    next_token = sample_next(predictions[0, -1, :])                         # use these to sample (get the index)\n",
    "                    sampled_token = TOKEN_INDEX[next_token]                                # use the index to pick the token\n",
    "                    sentence += sampled_token                                               # add it to our sentence\n",
    "                print(sentence)\n",
    "                print()\n",
    "            print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vicMAokx8uEB",
   "metadata": {
    "id": "vicMAokx8uEB"
   },
   "outputs": [],
   "source": [
    "# running 200 passes on model1 (initial model)\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "model1.fit(\n",
    "    lm_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=DATASET_LENGTH // (SEQUENCE_LENGTH + 1) // BATCH_SIZE,\n",
    "    # callbacks=[text_gen_callback, ckpt_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "-n2S8IPQVD8P",
   "metadata": {
    "id": "-n2S8IPQVD8P"
   },
   "outputs": [],
   "source": [
    "# model1.save('model1-200pass.keras')\n",
    "local = tf.keras.models.load_model('./content/model1-200pass.keras')\n",
    "# model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ku7y2rOlRubw",
   "metadata": {
    "id": "ku7y2rOlRubw"
   },
   "outputs": [],
   "source": [
    "def generate(sentence=\" \", generate_length=1, temperature=1., model=None):\n",
    "\n",
    "  for i in range(generate_length):\n",
    "      tokenized_sentence = text_vectorization([sentence])                       # encode the sentence & feed to the model\n",
    "      predictions = model(tokenized_sentence[:, - SEQUENCE_LENGTH + 1:])        # which gives us predictions  (crop to seq_len!)\n",
    "      next_token = sample_next(predictions[0, -1, :], temperature)              # use these to sample (get the index)\n",
    "      sampled_token = TOKEN_INDEX[next_token]                                   # use the index to pick the token\n",
    "      sentence += sampled_token\n",
    "      if len(sentence.split()) > 17:\n",
    "        break\n",
    "  return sentence\n",
    "\n",
    "def outputhaiku(sentence):\n",
    "  nlwords = ['i','he', 'she', 'it', 'they', 'for', 'and', 'nor', 'but', 'or', 'yet', 'so', 'the']  # list of linking and transition words\n",
    "  words = sentence.split()\n",
    "  count = 0\n",
    "  lineone = \"\"\n",
    "  linetwo = \"\"\n",
    "  linethree = \"\"\n",
    "  while len(lineone.split()) != 5:\n",
    "    if words[count] in nlwords and len(lineone.split()) >= 4:\n",
    "      break\n",
    "    lineone += words[count] + \" \"\n",
    "    count += 1\n",
    "  print(lineone)\n",
    "  while len(linetwo.split()) != 7:\n",
    "    if words[count] in nlwords and len(linetwo.split()) >= 6:\n",
    "      break\n",
    "    linetwo += words[count] + \" \"\n",
    "    count += 1\n",
    "  print(linetwo)\n",
    "  while len(linethree.split()) != 5:\n",
    "    if words[count] in nlwords and len(linethree.split()) >= 4:\n",
    "      break\n",
    "    linethree += words[count] + \" \"\n",
    "    count += 1\n",
    "  print(linethree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "jFfdu2s1YQlz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jFfdu2s1YQlz",
    "outputId": "a8903b1b-fa78-4fd5-f474-eeffb1be8b19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she said i always wanted \n",
      "to be a boy anyway they couldnt \n",
      "tell anything about me \n"
     ]
    }
   ],
   "source": [
    "sentence = generate(sentence=\"she \", generate_length=150, temperature=0.2, model = model1)\n",
    "outputhaiku(sentence=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "o-M6TXyX79Zs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-M6TXyX79Zs",
    "outputId": "56cd4978-0755-49c6-8435-7f36466dab09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what creek you know \n",
      "the name of that creek oh that \n",
      "creek yeah up that creek \n"
     ]
    }
   ],
   "source": [
    "sentence = generate(sentence=\"what \", generate_length=300, temperature=.5, model = model1)\n",
    "outputhaiku(sentence=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "mYxQ2dCQ_T4X",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mYxQ2dCQ_T4X",
    "outputId": "c532ca24-5f1c-49de-b34c-b43bc7a60224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pain the curtain youre direction \n",
      "four twenty ats saleant out straight \n",
      "for the house and carried \n"
     ]
    }
   ],
   "source": [
    "sentence = generate(sentence=\"pain \", generate_length=300, temperature=1.7, model = local)\n",
    "outputhaiku(sentence=sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jwD8hIICAHAU",
   "metadata": {
    "id": "jwD8hIICAHAU"
   },
   "source": [
    "# Takeaway from model1 \n",
    "- Using Hemingway's writing as data for poem-generating model is not ideal. The text is too short and concise, and they lack descriptive vocabs and phrasings.  \n",
    "- For poems-style text, it's better to generate with higher temperture to gives more weight to \"lower-probas\" words, for more random output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LvwNfKeyLv0e",
   "metadata": {
    "id": "LvwNfKeyLv0e"
   },
   "source": [
    "### 3. Different approach\n",
    "\n",
    "Implementing GloVe embeddings with a simple LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E0Iud6RjMNIk",
   "metadata": {
    "id": "E0Iud6RjMNIk"
   },
   "outputs": [],
   "source": [
    "def load_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DWTAI6NFMYES",
   "metadata": {
    "id": "DWTAI6NFMYES"
   },
   "outputs": [],
   "source": [
    "EMBED_DIM = 256\n",
    "LATENT_DIM = 2048\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "def build_model(embed_dim, latent_dim, num_heads, num_layers):\n",
    "    inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    x = PositionalEmbedding(SEQUENCE_LENGTH, VOCAB_SIZE, embed_dim)(inputs)\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerDecoder(embed_dim, latent_dim, num_heads)(inputs=x) # no encoder input!\n",
    "    outputs =tf.keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)    # probability distribution over the vocab\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "        metrics= [\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_model(EMBED_DIM, LATENT_DIM, NUM_HEADS, NUM_LAYERS)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
